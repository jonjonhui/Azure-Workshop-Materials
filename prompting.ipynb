{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# OpenAI Quickstart"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Install OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.7-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv\n",
            "  Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting requests>=2.20 (from openai)\n",
            "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm (from openai)\n",
            "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "Collecting aiohttp (from openai)\n",
            "  Using cached aiohttp-3.8.4-cp39-cp39-macosx_11_0_arm64.whl (338 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests>=2.20->openai)\n",
            "  Using cached charset_normalizer-3.1.0-cp39-cp39-macosx_11_0_arm64.whl (122 kB)\n",
            "Collecting idna<4,>=2.5 (from requests>=2.20->openai)\n",
            "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests>=2.20->openai)\n",
            "  Using cached urllib3-2.0.2-py3-none-any.whl (123 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests>=2.20->openai)\n",
            "  Using cached certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp->openai)\n",
            "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->openai)\n",
            "  Using cached multidict-6.0.4-cp39-cp39-macosx_11_0_arm64.whl (29 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai)\n",
            "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->openai)\n",
            "  Using cached yarl-1.9.2-cp39-cp39-macosx_11_0_arm64.whl (62 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->openai)\n",
            "  Using cached frozenlist-1.3.3-cp39-cp39-macosx_11_0_arm64.whl (35 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->openai)\n",
            "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: urllib3, tqdm, python-dotenv, multidict, idna, frozenlist, charset-normalizer, certifi, attrs, async-timeout, yarl, requests, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 attrs-23.1.0 certifi-2023.5.7 charset-normalizer-3.1.0 frozenlist-1.3.3 idna-3.4 multidict-6.0.4 openai-0.27.7 python-dotenv-1.0.0 requests-2.31.0 tqdm-4.65.0 urllib3-2.0.2 yarl-1.9.2\n",
            "Collecting plotly\n",
            "  Using cached plotly-5.14.1-py2.py3-none-any.whl (15.3 MB)\n",
            "Collecting scikit-learn\n",
            "  Using cached scikit_learn-1.2.2-cp39-cp39-macosx_12_0_arm64.whl (8.5 MB)\n",
            "Collecting matplotlib\n",
            "  Using cached matplotlib-3.7.1-cp39-cp39-macosx_11_0_arm64.whl (7.3 MB)\n",
            "Collecting pandas\n",
            "  Using cached pandas-2.0.1-cp39-cp39-macosx_11_0_arm64.whl (10.9 MB)\n",
            "Collecting tenacity>=6.2.0 (from plotly)\n",
            "  Using cached tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: packaging in /Users/jonathanhui/miniforge3/envs/azure_workshop2/lib/python3.9/site-packages (from plotly) (23.1)\n",
            "Collecting numpy>=1.17.3 (from scikit-learn)\n",
            "  Using cached numpy-1.24.3-cp39-cp39-macosx_11_0_arm64.whl (13.9 MB)\n",
            "Collecting scipy>=1.3.2 (from scikit-learn)\n",
            "  Using cached scipy-1.10.1-cp39-cp39-macosx_12_0_arm64.whl (28.9 MB)\n",
            "Collecting joblib>=1.1.1 (from scikit-learn)\n",
            "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
            "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Using cached contourpy-1.0.7-cp39-cp39-macosx_11_0_arm64.whl (229 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Using cached fonttools-4.39.4-py3-none-any.whl (1.0 MB)\n",
            "Collecting kiwisolver>=1.0.1 (from matplotlib)\n",
            "  Using cached kiwisolver-1.4.4-cp39-cp39-macosx_11_0_arm64.whl (63 kB)\n",
            "Collecting pillow>=6.2.0 (from matplotlib)\n",
            "  Using cached Pillow-9.5.0-cp39-cp39-macosx_11_0_arm64.whl (3.1 MB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
            "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/jonathanhui/miniforge3/envs/azure_workshop2/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
            "Collecting importlib-resources>=3.2.0 (from matplotlib)\n",
            "  Using cached importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
            "Collecting tzdata>=2022.1 (from pandas)\n",
            "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /Users/jonathanhui/miniforge3/envs/azure_workshop2/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /Users/jonathanhui/miniforge3/envs/azure_workshop2/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Installing collected packages: pytz, tzdata, threadpoolctl, tenacity, pyparsing, pillow, numpy, kiwisolver, joblib, importlib-resources, fonttools, cycler, scipy, plotly, pandas, contourpy, scikit-learn, matplotlib\n",
            "Successfully installed contourpy-1.0.7 cycler-0.11.0 fonttools-4.39.4 importlib-resources-5.12.0 joblib-1.2.0 kiwisolver-1.4.4 matplotlib-3.7.1 numpy-1.24.3 pandas-2.0.1 pillow-9.5.0 plotly-5.14.1 pyparsing-3.0.9 pytz-2023.3 scikit-learn-1.2.2 scipy-1.10.1 tenacity-8.2.2 threadpoolctl-3.1.0 tzdata-2023.3\n"
          ]
        }
      ],
      "source": [
        "!pip install openai python-dotenv\n",
        "!pip install plotly scikit-learn matplotlib pandas"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Overview  \n",
        "\"Large language models are functions that map text to text. Given an input string of text, a large language model tries to predict the text that will come next\"(1). This \"quickstart\" notebook will introduce users to high-level LLM concepts, core package requirements for getting started with AML, an soft introduction to prompt design, and severa short examples of different use cases.  \n",
        "\n",
        "For more quickstart examples please refer to the official Azure Open AI Quickstart Documentation https://learn.microsoft.com/en-us/azure/cognitive-services/openai/quickstart?pivots=programming-language-studio"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Getting started with Azure OpenAI Service\n",
        "\n",
        "New customers will need to [apply for access](https://aka.ms/oai/access) to Azure OpenAI Service.  \n",
        "After approval is complete, customers can log into the Azure portal, create an Azure OpenAI Service resource, and start experimenting with models via the studio  \n",
        "\n",
        "[Great resource for getting started quickly](https://techcommunity.microsoft.com/t5/educator-developer-blog/azure-openai-is-now-generally-available/ba-p/3719177 )\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Build your first prompt  \n",
        "This short exercise will provide a basic introduction for submitting prompts to an OpenAI model for a simple task \"summarization\".  \n",
        "\n",
        "![](images/generative-AI-models-reduced.jpg)  \n",
        "\n",
        "\n",
        "**Steps**:  \n",
        "1. Install OpenAI library in your python environment  \n",
        "2. Load standard helper libraries and set your typical OpenAI security credentials for the OpenAI Service that you've created  \n",
        "3. Choose a model for your task  \n",
        "4. Create a simple prompt for the model  \n",
        "5. Submit your request to the model API!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 2. Import helper libraries and instantiate credentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1674829434433
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "openai.api_type = \"azure\"\n",
        "openai.api_version = \"2022-12-01\"\n",
        "\n",
        "API_KEY = os.getenv(\"OPENAI_API_KEY\",\"\").strip() # replace this with your OpenAI API Key\n",
        "openai.api_key = API_KEY\n",
        "\n",
        "RESOURCE_ENDPOINT = os.getenv(\"OPENAI_API_BASE\",\"\").strip() # replace this with your OpenAI API Endpoint\n",
        "openai.api_base = RESOURCE_ENDPOINT"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 3. Finding the right model  \n",
        "The GPT-3 models can understand and generate natural language. The service offers four model capabilities, each with different levels of power and speed suitable for different tasks. Davinci is the most capable model, while Ada is the fastest. The following list represents the latest versions of GPT-3 models, ordered by increasing capability(1).  \n",
        "\n",
        "* text-ada-001\n",
        "* text-babbage-001\n",
        "* text-curie-001\n",
        "* text-davinci-003  \n",
        "\n",
        "[Azure OpenAI models](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/models])  \n",
        "![](images/a-b-c-d-models-reduced.jpg)  \n",
        "\n",
        "| | Similarity embedding | Text search embedding | Code Search Embedding |\n",
        "| --- | --- | --- |\n",
        "| |These models are good at capturing **semantic similarity** between two or more pieces of text. | These models help measure whether long documents are relevant to a short search query. There are two input types supported by this family: **doc**, for embedding the documents to be retrieved, and **query**, for embedding the search query. |Similar to text search embedding models, there are two input types supported by this family: **code**, for embedding code snippets to be retrieved, and text, for embedding natural language search queries. | \n",
        "|**Use cases** | Clustering, regression, anomaly detection, visualization | Search, context relevance, information retrieval | Code search and relevance |\n",
        "|**Models** |text-similarity-ada-001 <br> text-similarity-babbage-001 <br> text-similarity-curie-001 <br> text-similarity-davinci-001 | text-search-ada-doc-001  <br> text-search-ada-query-001  <br> text-search-babbage-doc-001  <br> text-search-babbage-query-001  <br> text-search-curie-doc-001  <br>text-search-curie-query-001  <br> text-search-davinci-doc-001  <br> text-search-davinci-query-001 | code-search-ada-code-001 <br> code-search-ada-text-001 <br> code-search-babbage-code-001 <br> code-search-babbage-text-001 | \n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Model Taxonomy  \n",
        "Let's choose a general text GPT-3 model, using the second most powerful model (Curie)\n",
        "\n",
        "**Model taxonomy**: {family} - {capability} - {input-type} - {identifier}  \n",
        "\n",
        "{family}     --> text   (general text GPT-3 model)  \n",
        "{capability} --> curie  (curie is second most powerful in ada-babbage-curie-davinci family)  \n",
        "{input-type} --> n/a    (only specified for search models)  \n",
        "{identifier} --> 001    (version 001)  \n",
        "\n",
        "model = \"text-curie-001\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1674742720788
        },
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Select the General Purpose davinci model for text\n",
        "model = \"text-davinci-003\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 4. Prompt Design  \n",
        "\n",
        "\"The magic of large language models is that by being trained to minimize this prediction error over vast quantities of text, the models end up learning concepts useful for these predictions. For example, they learn concepts like\"(1):\n",
        "\n",
        "* how to spell\n",
        "* how grammar works\n",
        "* how to paraphrase\n",
        "* how to answer questions\n",
        "* how to hold a conversation\n",
        "* how to write in many languages\n",
        "* how to code\n",
        "* etc.\n",
        "\n",
        "#### How to control a large language model  \n",
        "\"Of all the inputs to a large language model, by far the most influential is the text prompt(1).\n",
        "\n",
        "Large language models can be prompted to produce output in a few ways:\n",
        "\n",
        "Instruction: Tell the model what you want\n",
        "Completion: Induce the model to complete the beginning of what you want\n",
        "Demonstration: Show the model what you want, with either:\n",
        "A few examples in the prompt\n",
        "Many hundreds or thousands of examples in a fine-tuning training dataset\"\n",
        "\n",
        "\n",
        "\n",
        "#### There are three basic guidelines to creating prompts:\n",
        "\n",
        "**Show and tell**. Make it clear what you want either through instructions, examples, or a combination of the two. If you want the model to rank a list of items in alphabetical order or to classify a paragraph by sentiment, show it that's what you want.\n",
        "\n",
        "**Provide quality data**. If you're trying to build a classifier or get the model to follow a pattern, make sure that there are enough examples. Be sure to proofread your examples — the model is usually smart enough to see through basic spelling mistakes and give you a response, but it also might assume this is intentional and it can affect the response.\n",
        "\n",
        "**Check your settings.** The temperature and top_p settings control how deterministic the model is in generating a response. If you're asking it for a response where there's only one right answer, then you'd want to set these lower. If you're looking for more diverse responses, then you might want to set them higher. The number one mistake people use with these settings is assuming that they're \"cleverness\" or \"creativity\" controls.\n",
        "\n",
        "\n",
        "Source: https://github.com/Azure/OpenAI/blob/main/How%20to/Completions.md"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "![](images/prompt_design.jpg)\n",
        "image is creating your first text prompt!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 5. Submit!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1674494935186
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create your first prompt\n",
        "text_prompt = \"Should oxford commas always be used?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1674494938225
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<OpenAIObject text_completion id=cmpl-7JIGeNk3rTbj1tt58bGu3WF82qa1T at 0x10bca8b30> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"stop\",\n",
              "      \"index\": 0,\n",
              "      \"logprobs\": null,\n",
              "      \"text\": \"\\n\\nNo, the use of an oxford comma is a matter of preference, so it is not always necessary to use one.\"\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1684832808,\n",
              "  \"id\": \"cmpl-7JIGeNk3rTbj1tt58bGu3WF82qa1T\",\n",
              "  \"model\": \"text-davinci-003\",\n",
              "  \"object\": \"text_completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 27,\n",
              "    \"prompt_tokens\": 9,\n",
              "    \"total_tokens\": 36\n",
              "  }\n",
              "}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Simple API Call\n",
        "openai.Completion.create(\n",
        "    engine=model,\n",
        "    prompt=text_prompt,\n",
        "    max_tokens=60\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Repeat the same call, how do the results compare?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1674494940872
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<OpenAIObject text_completion id=cmpl-7JIGhJTN20gAxn9ofpgLyHlAHM4Ap at 0x104d97e00> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"stop\",\n",
              "      \"index\": 0,\n",
              "      \"logprobs\": null,\n",
              "      \"text\": \"\\n\\nNo, oxford commas are not always necessary. They should generally be used when a sentence contains a series of three or more items to help avoid ambiguity, but it is ultimately up to the writer's discretion.\"\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1684832811,\n",
              "  \"id\": \"cmpl-7JIGhJTN20gAxn9ofpgLyHlAHM4Ap\",\n",
              "  \"model\": \"text-davinci-003\",\n",
              "  \"object\": \"text_completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 45,\n",
              "    \"prompt_tokens\": 9,\n",
              "    \"total_tokens\": 54\n",
              "  }\n",
              "}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "openai.Completion.create(\n",
        "    engine=model,\n",
        "    prompt=text_prompt,\n",
        "    max_tokens=60\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Exercises for several use cases  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Summarize Text  \n",
        "#### Challenge  \n",
        "Summarize text by adding a 'tl;dr:' to the end of a text passage. Notice how the model understands how to perform a number of tasks with no additional instructions. You can experiment with more descriptive prompts than tl;dr to modify the model’s behavior and customize the summarization you receive(3).  \n",
        "\n",
        "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. \n",
        "\n",
        "\n",
        "\n",
        "Tl;dr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1674495198534
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "prompt = \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches.\\n\\nTl;dr\"\n",
        "\n",
        "model = \"text-davinci-003\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1674495201868
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"length\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"text\": \": We show that scaling up language models improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. This allows us to perform new language tasks from only a few examples or from simple instructions, something which current N\"\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1684826745,\n",
            "  \"id\": \"cmpl-7JGgrGs5x68jYa0KgkopgadtRAZLS\",\n",
            "  \"model\": \"text-davinci-003\",\n",
            "  \"object\": \"text_completion\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 60,\n",
            "    \"prompt_tokens\": 142,\n",
            "    \"total_tokens\": 202\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "#Setting a few additional, typical parameters during API Call\n",
        "response = openai.Completion.create(\n",
        "  engine=model,\n",
        "  prompt=prompt,\n",
        "  temperature=0.7,\n",
        "  max_tokens=60,\n",
        "  top_p=1,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0,\n",
        "  stop=None)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1674495223936
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<OpenAIObject text_completion id=cmpl-7JGgrGs5x68jYa0KgkopgadtRAZLS at 0x105810a90> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"length\",\n",
              "      \"index\": 0,\n",
              "      \"logprobs\": null,\n",
              "      \"text\": \": We show that scaling up language models improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. This allows us to perform new language tasks from only a few examples or from simple instructions, something which current N\"\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1684826745,\n",
              "  \"id\": \"cmpl-7JGgrGs5x68jYa0KgkopgadtRAZLS\",\n",
              "  \"model\": \"text-davinci-003\",\n",
              "  \"object\": \"text_completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 60,\n",
              "    \"prompt_tokens\": 142,\n",
              "    \"total_tokens\": 202\n",
              "  }\n",
              "}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Classify Text  \n",
        "#### Challenge  \n",
        "Classify items into categories provided at inference time. In the following example we provide both the categories and the text to classify in the prompt(*playground_reference). \n",
        "\n",
        "Customer Inquiry: Hello, one of the keys on my laptop keyboard broke recently and I'll need a replacement:\n",
        "\n",
        "Classified category:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1674499424645
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "prompt = \"Classify the following inquiry into one of the following: categories: [Pricing, Hardware Support, Software Support]\\n\\ninquiry: Hello, one of the keys on my laptop keyboard broke recently and I'll need a replacement:\\n\\nClassified category:\"\n",
        "\n",
        "model = \"text-davinci-003\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1674499378518
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"text\": \" Hardware Support\"\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1684826782,\n",
            "  \"id\": \"cmpl-7JGhS6ozQS2R9mee8754BSbIDRmF2\",\n",
            "  \"model\": \"text-davinci-003\",\n",
            "  \"object\": \"text_completion\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 2,\n",
            "    \"prompt_tokens\": 54,\n",
            "    \"total_tokens\": 56\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "response = openai.Completion.create(\n",
        "  engine=model,\n",
        "  prompt=prompt,\n",
        "  temperature=0,\n",
        "  max_tokens=60,\n",
        "  top_p=1,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0,\n",
        "  stop=None)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Generate New Product Names\n",
        "#### Challenge\n",
        "Create product names from examples words. Here we include in the prompt information about the product we are going to generate names for. We also provide a similar example to show the pattern we wish to receive. We have also set the temperature value high to increase randomness and more innovative responses.\n",
        "\n",
        "Product description: A home milkshake maker\n",
        "Seed words: fast, healthy, compact.\n",
        "Product names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\n",
        "\n",
        "Product description: A pair of shoes that can fit any foot size.\n",
        "Seed words: adaptable, fit, omni-fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1674257087279
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "prompt = \"Product description: A home milkshake maker\\nSeed words: fast, healthy, compact.\\nProduct names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\\n\\nProduct description: A pair of shoes that can fit any foot size.\\nSeed words: adaptable, fit, omni-fit.\"\n",
        "\n",
        "model = \"text-davinci-003\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "response = openai.Completion.create(\n",
        "  engine=model,\n",
        "  prompt=prompt,\n",
        "  temperature=0.8,\n",
        "  max_tokens=60,\n",
        "  top_p=1,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0,\n",
        "  stop=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1674495299501
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<OpenAIObject text_completion id=cmpl-7JGhfrAl6JIa3gVM9ZiaPDviawO7S at 0x1042d89f0> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"stop\",\n",
              "      \"index\": 0,\n",
              "      \"logprobs\": null,\n",
              "      \"text\": \"\\nProduct names: OmniFits, AdaptShoes, FitAlls, AdaptAlls.\"\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1684826795,\n",
              "  \"id\": \"cmpl-7JGhfrAl6JIa3gVM9ZiaPDviawO7S\",\n",
              "  \"model\": \"text-davinci-003\",\n",
              "  \"object\": \"text_completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 20,\n",
              "    \"prompt_tokens\": 69,\n",
              "    \"total_tokens\": 89\n",
              "  }\n",
              "}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Code Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = openai.Completion.create(\n",
        "  engine=os.getenv('DEPLOYMENT_NAME'),\n",
        "  prompt=\"Can you explain what does this code do?\\n#\\n# ###\\n\\\n",
        "   Code:\\n\\\n",
        "   SELECT d.name FROM Department d JOIN Employee e ON d.id = e.department_id WHERE e.id IN (SELECT employee_id FROM Salary_Payments WHERE date > now() - interval '3 months') GROUP BY d.name HAVING COUNT(*) > 10\\n#\\n#\\\n",
        "   Answer:\\n# \",\n",
        "   max_tokens=250,\n",
        "  stop=[\"#\",\";\"])\n",
        "\n",
        "print(response.choices[0].text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = openai.Completion.create(\n",
        "  engine=os.getenv('DEPLOYMENT_NAME'),\n",
        "  prompt=\"Can you explain what does this code do?\\n#\\n# ###\\n\\\n",
        "   Code:\\n\\\n",
        "   print('Hello World')\\n\\\n",
        "   Answer:\\n# \",\n",
        "   max_tokens=250,\n",
        "  stop=[\"#\",\";\"])\n",
        "\n",
        "print(response.choices[0].text)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# References  \n",
        "-Azure Reference Documentation  \n",
        "-Azure OpenAI GitHub Repo\n",
        "-cookbooks  \n",
        "-OpenAI website  \n",
        "\n",
        "1 - [Openai Cookbook](https://github.com/openai/openai-cookbook)  \n",
        "2 - [Azure Documentation - Azure Open AI Models](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/models)  \n",
        "3 - [OpenAI Studio Examples](https://oai.azure.com/portal)  \n",
        "4 - [[PUBLIC] Best practices for fine-tuning GPT-3 to classify text](https://docs.google.com/document/d/1rqj7dkuvl7Byd5KQPUJRxc19BJt8wo0yHNwK84KfU3Q/edit#)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# For More Help  \n",
        "[OpenAI Commercialization Team](AzureOpenAITeam@microsoft.com)  \n",
        "AI Specialized CSAs [aka.ms/airangers](aka.ms/airangers)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Contributors\n",
        "* Brandon Cowen\n",
        "* Ashish Chauhun\n",
        "* Louis Li  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
